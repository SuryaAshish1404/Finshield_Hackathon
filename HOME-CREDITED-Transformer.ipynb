{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667270b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME CREDIT — loaded.\n",
      "columns: ['DEFAULT', 'TARGET', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE'] ...\n",
      "categoricals: ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE']\n",
      "numericals: ['TARGET', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'OWN_CAR_AGE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'BUREAU_COUNT', 'PREVAPP_COUNT', 'POS_COUNT', 'INST_COUNT', 'CC_COUNT']\n",
      "example token_maps key: CODE_GENDER\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Inference.py — loads model artifacts, schema.json, generates synthetic rows.\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "import types\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def load_notebook_module(nb_path: Path, module_name: str = \"finshield_nb\") -> types.ModuleType:\n",
    "    import nbformat\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "    nb = nbformat.read(str(nb_path), as_version=4)\n",
    "    module = types.ModuleType(module_name)\n",
    "    shell = InteractiveShell.instance()\n",
    "    exec_env = module.__dict__\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == \"code\":\n",
    "            code = shell.input_transformer_manager.transform_cell(cell.source)\n",
    "            exec(code, exec_env)\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "class _RenameUnpickler(pickle.Unpickler):\n",
    "    def __init__(self, file, simple_tokenizer_cls):\n",
    "        super().__init__(file)\n",
    "        self._st_cls = simple_tokenizer_cls\n",
    "\n",
    "    def find_class(self, module, name):\n",
    "        if name == \"SimpleTokenizer\":\n",
    "            return self._st_cls\n",
    "        return super().find_class(module, name)\n",
    "\n",
    "\n",
    "def _load_tokenizer(path: Path, simple_tokenizer_cls):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except AttributeError:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return _RenameUnpickler(f, simple_tokenizer_cls).load()\n",
    "\n",
    "\n",
    "def sync_globals(nbmod, schema: dict, meta: dict):\n",
    "    \"\"\"Inject schema + token maps into notebook module globals.\"\"\"\n",
    "    nbmod.token_maps = meta.get(\"token_maps\", {})\n",
    "    nbmod.column_order = schema.get(\"columns\", [])\n",
    "    nbmod.categorical_columns = schema.get(\"categoricals\", [])\n",
    "    nbmod.numerical_columns = schema.get(\"numericals\", [])\n",
    "\n",
    "\n",
    "def build_token_constraints(tokenizer, meta):\n",
    "    token_constraints = {}\n",
    "    token_maps = meta.get(\"token_maps\", {})\n",
    "    for col, vmap in token_maps.items():\n",
    "        key = f\"<CAT_{col}>\"\n",
    "        allowed_ids = []\n",
    "        for tok in vmap.values():\n",
    "            tid = tokenizer.token2id.get(tok)\n",
    "            if tid is not None:\n",
    "                allowed_ids.append(tid)\n",
    "        token_constraints[key] = allowed_ids\n",
    "    return token_constraints\n",
    "\n",
    "\n",
    "def postprocess_dataframe(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    cols = schema.get(\"columns\", list(df.columns))\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    for c in schema.get(\"categoricals\", []):\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].replace({\"__UNK__\": pd.NA}).astype(\"string\")\n",
    "\n",
    "    for n in schema.get(\"numericals\", []):\n",
    "        if n in df.columns:\n",
    "            df[n] = pd.to_numeric(df[n], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def resolve_artifact_paths(model_dir: Path, dataset_name: str):\n",
    "    tok_path = model_dir / f\"{dataset_name}_tokenizer.pkl\"\n",
    "    meta_path = model_dir / \"meta.json\"\n",
    "    model_candidates = [\n",
    "        model_dir / f\"{dataset_name}_transformer_model.pt\",\n",
    "        model_dir / f\"{dataset_name}_transformer.pt\",\n",
    "    ]\n",
    "    mdl_path = next((p for p in model_candidates if p.exists()), None)\n",
    "    return tok_path, mdl_path, meta_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--models_dir\", type=Path, default=Path(\"models\"))\n",
    "    ap.add_argument(\"--dataset_name\", required=True)\n",
    "    ap.add_argument(\"--nb_path\", type=Path, required=True)\n",
    "    ap.add_argument(\"--n_rows\", type=int, default=1000)\n",
    "    ap.add_argument(\"--max_len\", type=int, default=1024)\n",
    "    ap.add_argument(\"--out\", type=Path, default=Path(\"data/processed/synthetic_output.csv\"))\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    nbmod = load_notebook_module(args.nb_path, module_name=\"finshield_nb\")\n",
    "    GPTMini = nbmod.GPTMini\n",
    "    SimpleTokenizer = nbmod.SimpleTokenizer\n",
    "    generate_synthetic_row = nbmod.generate_synthetic_row\n",
    "\n",
    "    # Resolve artifact paths\n",
    "    model_dir = args.models_dir / args.dataset_name\n",
    "    tok_path, mdl_path, meta_path = resolve_artifact_paths(model_dir, args.dataset_name)\n",
    "\n",
    "    if not tok_path.exists():\n",
    "        raise FileNotFoundError(f\"Tokenizer not found: {tok_path}\")\n",
    "    if mdl_path is None:\n",
    "        raise FileNotFoundError(f\"Model weights not found for {args.dataset_name}\")\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Meta not found: {meta_path}\")\n",
    "\n",
    "    # Load tokenizer & meta\n",
    "    tokenizer = _load_tokenizer(tok_path, SimpleTokenizer)\n",
    "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    # Load schema.json from processed dataset\n",
    "    schema_path = Path(\"data/processed\") / args.dataset_name / \"schema.json\"\n",
    "    if not schema_path.exists():\n",
    "        raise FileNotFoundError(f\"Schema not found: {schema_path}\")\n",
    "    schema = json.loads(schema_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    # Sync schema+meta into notebook module\n",
    "    sync_globals(nbmod, schema, meta)\n",
    "\n",
    "    # Load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GPTMini(vocab_size=len(tokenizer.token2id)).to(device)\n",
    "    state = torch.load(mdl_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    token_constraints = build_token_constraints(tokenizer, meta)\n",
    "\n",
    "    # Generate rows\n",
    "    rows = []\n",
    "    for _ in tqdm(range(args.n_rows), desc=f\"Generating {args.dataset_name} rows\"):\n",
    "        r = generate_synthetic_row(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            token_constraints=token_constraints,\n",
    "            max_len=args.max_len,\n",
    "        )\n",
    "        if r:\n",
    "            rows.append(r)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = postprocess_dataframe(df, schema)\n",
    "    args.out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(args.out, index=False)\n",
    "    print(f\"Saved synthetic data: {args.out} | rows={len(df)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a3abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "token_counter = 0\n",
    "for col in categorical_columns:\n",
    "    unique_vals = df[col].unique()\n",
    "    mapping = {val: f\"T{token_counter + i}\" for i, val in enumerate(unique_vals)}\n",
    "    token_maps[col] = mapping\n",
    "    token_counter += len(mapping)\n",
    "\n",
    "BASE = 16\n",
    "SCALE = 100      # keep 2 decimals after log1p\n",
    "NUM_DIGITS = 6\n",
    "def number_to_tokens_fixed(value):\n",
    "    \"\"\"\n",
    "    Encode a numeric value into tokens:\n",
    "    - Apply log1p transform (to compress big values).\n",
    "    - Scale by SCALE (to preserve 2 decimals).\n",
    "    - Convert to BASE=16 digits, fixed NUM_DIGITS long.\n",
    "    - Always return: [sign] + NUM_DIGITS tokens.\n",
    "    \n",
    "    Missing -> ['P'] + ['NAN'] * NUM_DIGITS\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return ['P'] + ['NAN'] * NUM_DIGITS\n",
    "    \n",
    "    # sign\n",
    "    sign = 'P' if value >= 0 else 'N'\n",
    "    \n",
    "    # log1p compression + scaling\n",
    "    scaled = int(round(np.log1p(abs(float(value))) * SCALE))\n",
    "    \n",
    "    # encode in base-16 with fixed length\n",
    "    digits = []\n",
    "    for _ in range(NUM_DIGITS):\n",
    "        digits.insert(0, str(scaled % BASE))\n",
    "        scaled //= BASE\n",
    "    \n",
    "    return [sign] + digits\n",
    "\n",
    "def tokenize_row(row):\n",
    "    tokens = []\n",
    "    for col in column_order:\n",
    "        if col in categorical_columns:\n",
    "            tokens.append(f\"<CAT_{col}>\")\n",
    "            tokens.append(token_maps[col].get(row[col], \"T_NAN\"))\n",
    "        else:\n",
    "            tokens.append(f\"<NUM_{col}>\")\n",
    "            tokens.extend(number_to_tokens_fixed(row[col]))\n",
    "    tokens.append(\"<EOR>\")\n",
    "    return tokens\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token2id = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2}\n",
    "        self.id2token = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\"}\n",
    "        self.next_id = 3\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        for seq in sequences:\n",
    "            for token in seq:\n",
    "                if token not in self.token2id:\n",
    "                    self.token2id[token] = self.next_id\n",
    "                    self.id2token[self.next_id] = token\n",
    "                    self.next_id += 1\n",
    "\n",
    "    def encode(self, seq):\n",
    "        return [self.token2id[\"<BOS>\"]] + [self.token2id[t] for t in seq] + [self.token2id[\"<EOS>\"]]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.id2token[i] for i in ids if i not in (0, 1, 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360016e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\tabllm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer):\n",
    "        self.data = [tokenizer.encode(seq) for seq in sequences]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx][:-1])\n",
    "        y = torch.tensor(self.data[idx][1:])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# --- Model ---\n",
    "class CustomDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None):\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt2 = self.norm1(tgt + self.dropout1(attn_output))\n",
    "\n",
    "        # Feed-forward block\n",
    "        ff_output = self.linear2(self.dropout(F.relu(self.linear1(tgt2))))\n",
    "        tgt3 = self.norm2(tgt2 + self.dropout2(ff_output))\n",
    "\n",
    "        return tgt3\n",
    "\n",
    "class GPTMini(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, 512, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomDecoderLayer(d_model, nhead, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) + self.pos[:, :x.size(1)]\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)  # seq_len, batch, d_model\n",
    "\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(x.size(0)).to(x.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, tgt_mask=mask)\n",
    "\n",
    "        x = x.transpose(0, 1)  # batch, seq_len, d_model\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2adb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- add these constants somewhere near the top of the file (same module) ----\n",
    "BASE = 16\n",
    "SCALE = 100        # 2-decimal precision in log-space\n",
    "NUM_DIGITS = 6     # fixed digits after the sign\n",
    "TOKENS_PER_NUMBER = 1 + NUM_DIGITS\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_synthetic_row(model, tokenizer, device, token_constraints, max_len=256):\n",
    "    model.eval()\n",
    "    input_ids = [tokenizer.token2id[\"<BOS>\"]]\n",
    "    decoded_tokens = []\n",
    "    expecting_num = False\n",
    "    num_step = 0\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0, -1, :]  # [V]\n",
    "\n",
    "        if decoded_tokens:\n",
    "            last_token = decoded_tokens[-1]\n",
    "\n",
    "            if last_token.startswith(\"<CAT_\"):\n",
    "                # Only allow categorical VALUE tokens for that column\n",
    "                allowed_ids = token_constraints.get(last_token, [])\n",
    "                if not allowed_ids:\n",
    "                    return None\n",
    "                masked_logits = torch.full_like(logits, float('-inf'))\n",
    "                masked_logits[allowed_ids] = logits[allowed_ids]\n",
    "                logits = masked_logits\n",
    "\n",
    "            elif last_token.startswith(\"<NUM_\"):\n",
    "                # next tokens must be numeric (sign + digits)\n",
    "                expecting_num = True\n",
    "                num_step = 0\n",
    "\n",
    "            elif expecting_num:\n",
    "                # block categorical VALUE tokens while emitting numeric tokens\n",
    "                all_ids = set(range(len(tokenizer.token2id)))\n",
    "                disallowed_ids = set()\n",
    "                for cat_key in token_constraints:\n",
    "                    disallowed_ids.update(token_constraints[cat_key])\n",
    "                allowed_ids = list(all_ids - disallowed_ids)\n",
    "\n",
    "                masked_logits = torch.full_like(logits, float('-inf'))\n",
    "                masked_logits[allowed_ids] = logits[allowed_ids]\n",
    "                logits = masked_logits\n",
    "\n",
    "                num_step += 1\n",
    "                if num_step == TOKENS_PER_NUMBER:  # sign + NUM_DIGITS digits\n",
    "                    expecting_num = False\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "            return None\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        token_str = tokenizer.id2token.get(next_token, \"\")\n",
    "\n",
    "        if token_str in [\"<EOS>\", \"<EOR>\"]:\n",
    "            break\n",
    "\n",
    "        input_ids.append(next_token)\n",
    "        decoded_tokens.append(token_str)\n",
    "\n",
    "    # Decode tokens -> row dict\n",
    "    row = {}\n",
    "    i = 0\n",
    "    while i < len(decoded_tokens):\n",
    "        if decoded_tokens[i] == \"<EOR>\":\n",
    "            break\n",
    "        token = decoded_tokens[i]\n",
    "\n",
    "        if token.startswith(\"<CAT_\"):\n",
    "            current_col = token[5:-1]\n",
    "            i += 1\n",
    "            # reverse-lookup categorical token -> original string value\n",
    "            row[current_col] = next((k for k, v in token_maps[current_col].items()\n",
    "                                     if v == decoded_tokens[i]), None)\n",
    "\n",
    "        elif token.startswith(\"<NUM_\"):\n",
    "            current_col = token[5:-1]\n",
    "            i += 1\n",
    "            # need: sign + NUM_DIGITS digits\n",
    "            if i + NUM_DIGITS >= len(decoded_tokens):\n",
    "                return None\n",
    "            try:\n",
    "                sign_tok = decoded_tokens[i]\n",
    "                digit_toks = decoded_tokens[i+1 : i+1+NUM_DIGITS]\n",
    "\n",
    "                # if any 'NAN' digit was produced -> NaN\n",
    "                if any(dt == 'NAN' for dt in digit_toks):\n",
    "                    value = float(\"nan\")\n",
    "                else:\n",
    "                    acc = 0\n",
    "                    for d in digit_toks:\n",
    "                        acc = acc * BASE + int(d)\n",
    "                    # inverse of: scaled = round(log1p(abs(x)) * SCALE)\n",
    "                    # value ≈ sign * expm1(acc / SCALE)\n",
    "                    sign = 1 if sign_tok == 'P' else -1\n",
    "                    value = sign * torch.expm1(torch.tensor(acc / float(SCALE))).item()\n",
    "\n",
    "                row[current_col] = value\n",
    "            except Exception:\n",
    "                row[current_col] = float(\"nan\")\n",
    "\n",
    "            i += NUM_DIGITS  # consumed digits after the sign\n",
    "        i += 1\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9f4628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_generated_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Decode a flat list of tokens back into a row dict,\n",
    "    using Option B (log1p + fixed NUM_DIGITS digits).\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "\n",
    "        if token.startswith(\"<CAT_\"):\n",
    "            col = token[5:-1] if token.endswith(\">\") else token[5:]\n",
    "            i += 1\n",
    "            row[col] = next((k for k, v in token_maps[col].items()\n",
    "                             if v == tokens[i]), None)\n",
    "\n",
    "        elif token.startswith(\"<NUM_\"):\n",
    "            col = token[5:-1] if token.endswith(\">\") else token[5:]\n",
    "            i += 1\n",
    "            # Expect: sign + NUM_DIGITS digits\n",
    "            if i + NUM_DIGITS > len(tokens):\n",
    "                row[col] = float(\"nan\")\n",
    "                break\n",
    "\n",
    "            sign_tok = tokens[i]\n",
    "            digit_toks = tokens[i+1 : i+1+NUM_DIGITS]\n",
    "\n",
    "            if any(t == 'NAN' for t in digit_toks):\n",
    "                row[col] = float(\"nan\")\n",
    "            else:\n",
    "                try:\n",
    "                    acc = 0\n",
    "                    for d in digit_toks:\n",
    "                        acc = acc * BASE + int(d)\n",
    "                    sign = 1 if sign_tok == 'P' else -1\n",
    "                    # inverse transform\n",
    "                    val = sign * np.expm1(acc / float(SCALE))\n",
    "                    row[col] = float(val)\n",
    "                except Exception:\n",
    "                    row[col] = float(\"nan\")\n",
    "\n",
    "            i += NUM_DIGITS\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return row if len(row) == len(column_order) else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 4000/4000 [00:44<00:00, 90.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 275.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val Loss: 0.3673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 4000/4000 [00:46<00:00, 86.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 0.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 246.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val Loss: 0.3484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 4000/4000 [00:46<00:00, 86.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 0.3558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 263.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val Loss: 0.3399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 4000/4000 [00:47<00:00, 85.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 0.3468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 268.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val Loss: 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 4000/4000 [00:47<00:00, 84.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 0.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|██████████| 1000/1000 [00:05<00:00, 194.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val Loss: 0.3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 4000/4000 [00:54<00:00, 73.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 0.3377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 223.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val Loss: 0.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 4000/4000 [00:53<00:00, 75.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss: 0.3350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Val]: 100%|██████████| 1000/1000 [00:05<00:00, 196.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val Loss: 0.3261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 4000/4000 [00:56<00:00, 71.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss: 0.3329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Val]: 100%|██████████| 1000/1000 [00:05<00:00, 195.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Val Loss: 0.3244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 4000/4000 [00:53<00:00, 74.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss: 0.3313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 237.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Val Loss: 0.3238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 4000/4000 [00:53<00:00, 75.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss: 0.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 220.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Val Loss: 0.3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|██████████| 4000/4000 [00:51<00:00, 77.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss: 0.3288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 222.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Val Loss: 0.3221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|██████████| 4000/4000 [00:51<00:00, 77.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss: 0.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 255.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Val Loss: 0.3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|██████████| 4000/4000 [00:50<00:00, 79.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Loss: 0.3270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 253.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val Loss: 0.3208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|██████████| 4000/4000 [00:51<00:00, 78.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Loss: 0.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 208.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Val Loss: 0.3204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|██████████| 4000/4000 [00:49<00:00, 81.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train Loss: 0.3255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 216.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Val Loss: 0.3197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|██████████| 4000/4000 [00:50<00:00, 79.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train Loss: 0.3249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Val]: 100%|██████████| 1000/1000 [00:04<00:00, 212.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Val Loss: 0.3192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|██████████| 4000/4000 [43:46<00:00,  1.52it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train Loss: 0.3243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Val]: 100%|██████████| 1000/1000 [00:02<00:00, 334.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Val Loss: 0.3187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|██████████| 4000/4000 [00:41<00:00, 97.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train Loss: 0.3238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 300.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Val Loss: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|██████████| 4000/4000 [00:41<00:00, 96.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train Loss: 0.3234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 279.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Val Loss: 0.3181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Train]: 100%|██████████| 4000/4000 [00:46<00:00, 85.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train Loss: 0.3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 281.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Val Loss: 0.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Train]: 100%|██████████| 4000/4000 [00:42<00:00, 94.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train Loss: 0.3224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 312.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Val Loss: 0.3176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [Train]: 100%|██████████| 4000/4000 [00:43<00:00, 92.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Train Loss: 0.3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 261.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Val Loss: 0.3170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [Train]: 100%|██████████| 4000/4000 [00:43<00:00, 91.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Train Loss: 0.3217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 276.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Val Loss: 0.3166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [Train]: 100%|██████████| 4000/4000 [00:42<00:00, 94.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Train Loss: 0.3213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 263.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Val Loss: 0.3167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [Train]: 100%|██████████| 4000/4000 [00:45<00:00, 88.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Train Loss: 0.3210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 264.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Val Loss: 0.3163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 [Train]: 100%|██████████| 4000/4000 [00:41<00:00, 97.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Train Loss: 0.3207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 304.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Val Loss: 0.3167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 [Train]: 100%|██████████| 4000/4000 [00:42<00:00, 94.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Train Loss: 0.3204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 268.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Val Loss: 0.3161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 [Train]: 100%|██████████| 4000/4000 [00:41<00:00, 96.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Train Loss: 0.3201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 297.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Val Loss: 0.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 [Train]: 100%|██████████| 4000/4000 [00:42<00:00, 94.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Train Loss: 0.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 314.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Val Loss: 0.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 [Train]: 100%|██████████| 4000/4000 [00:44<00:00, 90.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Train Loss: 0.3197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 [Val]: 100%|██████████| 1000/1000 [00:03<00:00, 289.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Val Loss: 0.3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenized = df.apply(tokenize_row, axis=1).tolist()\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    tokenizer.fit(tokenized)\n",
    "\n",
    "    dataset = TabularDataset(tokenized, tokenizer)\n",
    "    # print(dataset[0])  # Check the first item\n",
    "    train_len = int(0.8 * len(dataset))\n",
    "    train_data, val_data = random_split(dataset, [train_len, len(dataset) - train_len])\n",
    "    pad = lambda x: tuple(nn.utils.rnn.pad_sequence(t, batch_first=True) for t in zip(*x))\n",
    "    train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=pad)\n",
    "    val_loader = DataLoader(val_data, batch_size=8, collate_fn=pad)\n",
    "\n",
    "    model = GPTMini(len(tokenizer.token2id)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out.view(-1, out.size(-1)), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Train Loss: {total / len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        vtotal = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = model(x)\n",
    "                loss = loss_fn(out.view(-1, out.size(-1)), y.view(-1))\n",
    "                vtotal += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Val Loss: {vtotal / len(val_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"home_credit_transformer_model.pt\")\n",
    "    with open(\"home_credit_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "__all__ = [\"GPTMini\", \"SimpleTokenizer\", \"token_maps\", \"column_order\", \"generate_synthetic_row\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a0c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported: GPTMini, SimpleTokenizer, generate_synthetic_row, schema globals\n"
     ]
    }
   ],
   "source": [
    "# === Exports for Inference ===\n",
    "# This cell ensures that Inference.py can just `import HOME_CREDITED_Transformer`\n",
    "\n",
    "__all__ = [\n",
    "    \"GPTMini\",\n",
    "    \"SimpleTokenizer\",\n",
    "    \"generate_synthetic_row\",\n",
    "    \"token_maps\",\n",
    "    \"column_order\",\n",
    "    \"categorical_columns\",\n",
    "    \"numerical_columns\",\n",
    "]\n",
    "\n",
    "print(\"✅ Exported: GPTMini, SimpleTokenizer, generate_synthetic_row, schema globals\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
